{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def dataframeToMongo(data, collectionName):\n",
    "    client = MongoClient('localhost',int(27017))\n",
    "    db = client[\"DataVizMgmt\"]\n",
    "    collection = db[collectionName]\n",
    "    my_list = data.to_dict('records')\n",
    "    l = len(my_list)\n",
    "    ran = range(l)\n",
    "    steps=ran[100::100]\n",
    "    steps.extend([l])\n",
    "    i = 0\n",
    "    for j in steps:\n",
    "        collection.insert_many(my_list[i:j]) # fill de collection\n",
    "        i = j\n",
    "\n",
    "class BigQueryHelper(object):\n",
    "    \"\"\"\n",
    "    Helper class to simplify common BigQuery tasks like executing queries,\n",
    "    showing table schemas, etc without worrying about table or dataset pointers.\n",
    "\n",
    "    See the BigQuery docs for details of the steps this class lets you skip:\n",
    "    https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/reference.html\n",
    "    \"\"\"\n",
    "\n",
    "    BYTES_PER_GB = 2**30\n",
    "\n",
    "    def __init__(self, active_project, dataset_name, max_wait_seconds=180):\n",
    "        self.project_name = active_project\n",
    "        self.dataset_name = dataset_name\n",
    "        self.max_wait_seconds = max_wait_seconds\n",
    "        self.client = bigquery.Client()\n",
    "        self.__dataset_ref = self.client.dataset(self.dataset_name, project=self.project_name)\n",
    "        self.dataset = None\n",
    "        self.tables = dict()  # {table name (str): table object}\n",
    "        self.__table_refs = dict()  # {table name (str): table reference}\n",
    "        self.total_gb_used_net_cache = 0\n",
    "\n",
    "    def __fetch_dataset(self):\n",
    "        # Lazy loading of dataset. For example,\n",
    "        # if the user only calls `self.query_to_pandas` then the\n",
    "        # dataset never has to be fetched.\n",
    "        if self.dataset is None:\n",
    "            self.dataset = self.client.get_dataset(self.__dataset_ref)\n",
    "\n",
    "    def __fetch_table(self, table_name):\n",
    "        # Lazy loading of table\n",
    "        self.__fetch_dataset()\n",
    "        if table_name not in self.__table_refs:\n",
    "            self.__table_refs[table_name] = self.dataset.table(table_name)\n",
    "        if table_name not in self.tables:\n",
    "            self.tables[table_name] = self.client.get_table(self.__table_refs[table_name])\n",
    "\n",
    "    def table_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Get the schema for a specific table from a dataset\n",
    "        \"\"\"\n",
    "        self.__fetch_table(table_name)\n",
    "        return(self.tables[table_name].schema)\n",
    "\n",
    "    def list_tables(self):\n",
    "        \"\"\"\n",
    "        List the names of the tables in a dataset\n",
    "        \"\"\"\n",
    "        self.__fetch_dataset()\n",
    "        return([x.table_id for x in self.client.list_dataset_tables(self.dataset)])\n",
    "\n",
    "    def estimate_query_size(self, query):\n",
    "        \"\"\"\n",
    "        Estimate gigabytes scanned by query.\n",
    "        Does not consider if there is a cached query table.\n",
    "        See https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.dryRun\n",
    "        \"\"\"\n",
    "        my_job_config = bigquery.job.QueryJobConfig()\n",
    "        my_job_config.dry_run = True\n",
    "        my_job = self.client.query(query, job_config=my_job_config)\n",
    "        return my_job.total_bytes_processed / self.BYTES_PER_GB\n",
    "\n",
    "    def query_to_pandas(self, query):\n",
    "        \"\"\"\n",
    "        Take a SQL query & return a pandas dataframe\n",
    "        \"\"\"\n",
    "        my_job = self.client.query(query)\n",
    "        iterator = my_job.result()\n",
    "        records = [el[0] for el in iterator]\n",
    "        return pd.DataFrame(records)\n",
    "        \n",
    "    def query_to_pandas_safe(self, query, max_gb_scanned=1):\n",
    "        \"\"\"\n",
    "        Execute a query if it's smaller than a certain number of gigabytes\n",
    "        \"\"\"\n",
    "        query_size = self.estimate_query_size(query)\n",
    "        if query_size <= max_gb_scanned:\n",
    "            return self.query_to_pandas(query)\n",
    "        msg = \"Query cancelled; estimated size of {0} exceeds limit of {1} GB\"\n",
    "        print(msg.format(query_size, max_gb_scanned))\n",
    "\n",
    "    def head(self, table_name, num_rows=5, start_index=None, selected_columns=None):\n",
    "        \"\"\"\n",
    "        Get the first n rows of a table as a DataFrame\n",
    "        \"\"\"\n",
    "        self.__fetch_table(table_name)\n",
    "        active_table = self.tables[table_name]\n",
    "        schema_subset = None\n",
    "        if selected_columns:\n",
    "            schema_subset = [col for col in active_table.schema if col.name in selected_columns]\n",
    "        results = self.client.list_rows(active_table, selected_fields=schema_subset,\n",
    "            max_results=num_rows, start_index=start_index)\n",
    "        results = [x for x in results]\n",
    "        return pd.DataFrame(\n",
    "            data=[list(x.values()) for x in results], columns=list(sorted(results[0]._xxx_field_to_index, key=results[0]._xxx_field_to_index.get)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"DataVizMgmt-8bd22854739a.json\"#\"DataVizMgmt-c847a9a5ec04.json\"\n",
    "\n",
    "nyc = BigQueryHelper(active_project=\"bigquery-public-data\",\n",
    "                                   dataset_name=\"new_york\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elaborating uber-raw-data-apr14.csv\n",
      "elaborating uber-raw-data-aug14.csv\n",
      "elaborating uber-raw-data-janjune-15.csv\n",
      "elaborating uber-raw-data-jul14.csv\n",
      "elaborating uber-raw-data-jun14.csv\n",
      "elaborating uber-raw-data-may14.csv\n",
      "elaborating uber-raw-data-sep14.csv\n"
     ]
    }
   ],
   "source": [
    "uberDataPath = \"uberData\"\n",
    "tables = os.listdir(uberDataPath)\n",
    "bqDfs = []\n",
    "for t in tables:\n",
    "    print 'elaborating', t\n",
    "    df = pd.read_csv(os.path.join(uberDataPath, t))\n",
    "    dataframeToMongo(df, t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elaborating tlc_yellow_trips_2015\n",
      "rows stored: 100000\n",
      "rows stored: 200000\n",
      "rows stored: 300000\n",
      "rows stored: 400000\n",
      "rows stored: 500000\n",
      "rows stored: 600000\n"
     ]
    }
   ],
   "source": [
    "tables = nyc.list_tables()\n",
    "bqDfs = []\n",
    "for t in tables:\n",
    "    if t in ['tlc_yellow_trips_2015'] :\n",
    "        print 'elaborating', t\n",
    "        rowsObject = nyc.query_to_pandas(\"SELECT COUNT(*) FROM `bigquery-public-data.new_york.%s`\"% t)\n",
    "        rows = rowsObject[0][0]\n",
    "        start = 700000\n",
    "        size= 100000\n",
    "        while (size):\n",
    "            if start + size > rows:\n",
    "                size = None\n",
    "                break\n",
    "            bqDf1 = nyc.head(t, num_rows=size, start_index=start)\n",
    "            dataframeToMongo(bqDf1, t)\n",
    "            \n",
    "            start = start + size\n",
    "            print 'rows stored:', start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost',int(27017))\n",
    "db = client[\"DataVizMgmt\"]\n",
    "collection2009 = db['tlc_yellow_trips_2009']\n",
    "collection2015 = db['tlc_yellow_trips_2015']\n",
    "taxiRides2009 = collection2009.count()\n",
    "taxiRides2015 = collection2015.count()\n",
    "perc = float((taxiRides2015 - taxiRides2009)) / taxiRides2009 * 100\n",
    "print \"Le corse dei taxi sono passate da %d \\\n",
    "    nel 2009 a %d nel 2015 (%f %%)\" % (taxiRides2009, taxiRides2015, perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean2009Cursor = collection2009.aggregate([{\"$group\": {\"_id\":None, \"amount\": {\"$avg\":\"$total_amount\"} } }])\n",
    "mean2015Cursor = collection2015.aggregate([{\"$group\": {\"_id\":None, \"amount\": {\"$avg\":\"$total_amount\"} } }])\n",
    "mean2009 = [el for el in mean2009Cursor][0]['amount']\n",
    "mean2015 = [el for el in mean2015Cursor][0]['amount']\n",
    "percMean = (mean2015 - mean2009) / mean2009 * 100\n",
    "print \"La spesa media per le corse taxi Ã¨ passata dal %f del 2009 al %f nel 2015 (%d %%)\" % (mean2009, mean2015, percMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridesWithTips2009 = collection2009.find({\"tip_amount\": {\"$gt\": 0}}).count()\n",
    "ridesWithTips2015 = collection2015.find({\"tip_amount\": {\"$gt\": 0}}).count()\n",
    "avgTips2009 = float(ridesWithTips2009) / taxiRides2009 * 100\n",
    "avgTips2015 = float(ridesWithTips2015) / taxiRides2015  * 100\n",
    "difference = avgTips2009  - avgTips2015\n",
    "print \"Le corse con mancia sono passate dal %f %% del 2009 al %f %% del 2015 (%f %%) \" % (avgTips2009, avgTips2015, abs(difference))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
